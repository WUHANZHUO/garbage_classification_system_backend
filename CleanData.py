import os
import imagehash
from PIL import Image
from collections import defaultdict

def find_and_remove_duplicate_images(dataset_path, remove_duplicates=True):
    """
    æ£€æµ‹æ•°æ®é›†ä¸­ç›¸åŒåˆ†ç±»ä¸‹çš„é‡å¤å›¾ç‰‡ï¼Œå¹¶å¯é€‰æ‹©åˆ é™¤é‡å¤é¡¹
    :param dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„ (e.g., 'dataset/')
    :param remove_duplicates: æ˜¯å¦åˆ é™¤é‡å¤å›¾ç‰‡ï¼Œé»˜è®¤ä¸ºTrue
    """
    duplicates_found = False
    total_removed = 0
    
    # éå†æ¯ä¸ªåˆ†ç±»ç›®å½•
    for class_name in os.listdir(dataset_path):
        class_path = os.path.join(dataset_path, class_name)
        
        if not os.path.isdir(class_path):
            continue
            
        print(f"\næ£€æŸ¥åˆ†ç±»: {class_name}")
        image_hashes = defaultdict(list)
        
        # éå†åˆ†ç±»ç›®å½•ä¸­çš„å›¾ç‰‡æ–‡ä»¶
        for filename in os.listdir(class_path):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
                img_path = os.path.join(class_path, filename)
                
                try:
                    with Image.open(img_path) as img:
                        # è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ (å¯æ›¿æ¢ä¸ºaverage_hash, dhashç­‰)
                        img_hash = str(imagehash.phash(img))
                        image_hashes[img_hash].append((filename, img_path))
                except Exception as e:
                    print(f"  è·³è¿‡æŸåå›¾ç‰‡: {filename} ({str(e)})")
        
        # æ£€æŸ¥é‡å¤é¡¹
        class_removed = 0
        for img_hash, file_info_list in image_hashes.items():
            if len(file_info_list) > 1:
                duplicates_found = True
                print(f"  âš ï¸ å‘ç°é‡å¤å›¾ç‰‡ (å“ˆå¸Œ: {img_hash}):")
                
                # æ˜¾ç¤ºæ‰€æœ‰é‡å¤æ–‡ä»¶
                for i, (filename, filepath) in enumerate(file_info_list):
                    if i == 0:
                        print(f"    - {filename} (ä¿ç•™)")
                    else:
                        print(f"    - {filename} (å¾…åˆ é™¤)")
                
                # åˆ é™¤é‡å¤é¡¹ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
                if remove_duplicates:
                    for i, (filename, filepath) in enumerate(file_info_list[1:], 1):
                        try:
                            os.remove(filepath)
                            print(f"    âœ… å·²åˆ é™¤: {filename}")
                            class_removed += 1
                            total_removed += 1
                        except Exception as e:
                            print(f"    âŒ åˆ é™¤å¤±è´¥: {filename} ({str(e)})")
        
        if class_removed > 0:
            print(f"  ğŸ“Š {class_name} åˆ†ç±»åˆ é™¤äº† {class_removed} å¼ é‡å¤å›¾ç‰‡")
    
    # æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
    if duplicates_found:
        if remove_duplicates:
            print(f"\nğŸ¯ æ¸…ç†å®Œæˆï¼æ€»å…±åˆ é™¤äº† {total_removed} å¼ é‡å¤å›¾ç‰‡")
        else:
            print(f"\nğŸ“‹ æ£€æµ‹å®Œæˆï¼å‘ç°é‡å¤å›¾ç‰‡ä½†æœªåˆ é™¤ï¼ˆremove_duplicates=Falseï¼‰")
    else:
        print("\nâœ… æœªå‘ç°é‡å¤å›¾ç‰‡")

def backup_dataset(dataset_path):
    """
    åˆ›å»ºæ•°æ®é›†å¤‡ä»½ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
    :param dataset_path: æ•°æ®é›†è·¯å¾„
    """
    import shutil
    backup_path = f"{dataset_path}_backup"
    
    if os.path.exists(backup_path):
        print(f"âš ï¸ å¤‡ä»½ç›®å½•å·²å­˜åœ¨: {backup_path}")
        return False
    
    try:
        shutil.copytree(dataset_path, backup_path)
        print(f"âœ… æ•°æ®é›†å·²å¤‡ä»½åˆ°: {backup_path}")
        return True
    except Exception as e:
        print(f"âŒ å¤‡ä»½å¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    dataset_root = "garbage-dataset"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„
    
    print("ğŸš€ å¼€å§‹æ¸…ç†æ•°æ®é›†é‡å¤å›¾ç‰‡...")
    find_and_remove_duplicate_images(dataset_root, remove_duplicates=True)